import Image from "next/image";
import Alert from "@/components/DocsAlert";
import Link from "next/link";

Firezone's approach to DNS works a bit differently than one might expect. One
question we get a lot is, "why do my DNS Resources resolve to different IPs with
Firezone enabled?". Great question. Let's explain that now!

What follows is a brief history of DNS, (a few of) the security issues that
plague it, and how Firezone's DNS-based traffic routing was designed to address
them.

## A brief history lesson

But before we dive into all the fun details, it's helpful to first understand
the problem DNS was originally designed to solve.

DNS was created in the early 1980s to address a classic scalability problem with
the early ARPANET.

<Image
  src="/images/blog/how-dns-based-traffic-routing-works/arpanet-logical-map.png"
  alt="Early ARPANET logical map"
  width={1200}
  height={1200}
  className="mx-auto rounded shadow"
/>

<p className="text-sm italic text-center mx-auto">
  Figure 1: Early ARPANET logical map
</p>

The ARPANET, the precursor to the modern internet, was a network of computers
that connected various research institutions and government agencies across the
United States. It was the first network to use the new TCP/IP protocol suite,
which provided an IP address to each host on the network.

To connect to a host on the ARPANET, you needed its IP address. But there was a
problem: IP addresses are hard to remember, and it turns out humans aren't great
at remembering long strings of numbers. To solve this, a simple system was
created to map each host's IP address to a human-friendly "hostname".

Simple enough, right? Well, not quite. The system was perhaps a bit _too_
simple.

### Files don't scale

Hostname to IP address mappings were stored in a single file, aptly named
`HOSTS.TXT`, and stored on the `SRI-NIC` server -- a system maintained by the
Network Information Center (NIC) at the Stanford Research Institute (SRI).

<p className="text-sm italic text-center mx-auto">
  Figure 2: Artist's rendition of the original HOSTS.TXT file
</p>

Whenever an ARPANET member organization (yes, you had to be a member at that
time) wanted to add a host to the file or update its IP address, it had to
contact the SRI (during standard business hours!) and file a formal request
containing the updated mapping.

Need to make an update over the weekend or on a US holiday? Too bad, you were
out of luck! The system worked ok when there were only a handful of hosts to
maintain. But, as you might imagine, it became increasingly unmanageable as more
hosts joined.

## Enter the Domain Name System (DNS)

Over the course of several meetings, early ARPANET engineers (who would later go
on to form the [IETF](https://www.ietf.org/)) devised a clever solution to the
scalability problem they were facing: instead of storing the hostname lookup
table in a single file, they would chop it up instead and distribute the entries
across several "Nameservers", each one responsible for maintaining the hosts
that belonged to a particular network, or "Domain".

Hence the name: the Domain Name System, or DNS for short.

### How DNS works

At a high level, DNS is a heirarchical system that distributes the
responsibility of resolving a a fully-qualified domain name (FQDN) to a series
of nameservers, each one responsible for resolving a different part.

<p className="text-sm italic text-center mx-auto">Figure 3: How DNS works</p>

Here's how it works:

1. An application makes a query. The first stop is the **stub resolver**, a
   small piece of software on the host that's responsible for resolving DNS
   queries.
1. The stub resolver typically maintains a small cache of recent queries. If the
   query misses the cache, it forwards the query to an **upstream resolver**.
   This is typically a larger, caching resolver run by your ISP or more recently
   a public DNS service like NextDNS or Cloudflare.
1. If _this_ query misses the cache, the upstream resolver begins the full
   process of resolution. It starts by forwarding the query to a **root
   nameserver**. The root nameserver is at the top of the DNS hierarchy.
1. The root nameserver responds to the upsteam resolver with the IP address of
   the **TLD nameserver** for the root in question, for example `com` or `net`.
   The upstream resolver then forwards the query to the TLD nameserver.
1. The TLD nameserver responds with the IP address of the **authoritative
   nameserver** for the domain in question. This is the nameserver the owner of
   the domain will configure to respond to queries for that domain. It's where
   you typically configure your DNS records for your domain, for example.
1. The upstream resolver then forwards the query to the authoritative
   nameserver.
1. Finally, the authoritative nameserver responds with the IP address of the
   host in question, and the upstream resolver returns the final answer to the
   stub resolver on the host that originally made the query.
1. The application on the host can now connect to the IP address returned by the
   stub resolver.

The system of root, TLD, and authoritative nameservers replace the function of
the original `HOSTS.TXT` file maintained by NIC in the ARPANET. However, now
it's distributed -- no one server or organization is responsible for maintaining
the entire database, and the system can scale as more hosts, domains, and IP
addresses are added.

On today's internet, the whole process for resolving an query usually takes only
a few hundred milliseconds. Caching resolvers help to speed up the process by
storing the results of queries for a certain amount of time, known as the
record's time-to-live (TTL).

This means that if a host makes the same query multiple times, the stub or
upstream resolver can return the result immediately (assuming the TTL hasn't
expired) without having to query the hierarchy of root, TLD, and authoritative
nameservers again. This can speed up query times by orders of magnitude, to the
point where stub resolvers responding with cached responses are nearly
instantaneous.

So hostnames are now mapped to IP addresses in a scalable way, and queries are
pretty fast on average. DNS worked quite well for the ARPANET! So well, in fact,
that it was adopted mostly unchanged by the internet as it grew to become the
global network we know and love today.

But hold your champagne bottles. There's just one problem with all these
delightfully distributed domain lookups: security.

### Security issues with DNS

The thing is, DNS was designed in the early 1980s, when the ARPANET was a small,
trusted network of research institutions and government agencies. The system was
designed with the assumption that all entities on the network were known and
trusted, and that the network itself was secure.

As the ARPANET grew to become the internet, however, this assumption no longer
held.

#### DNS spoofing

<p className="text-sm italic text-center mx-auto">
  Figure 4: An illustration of DNS spoofing
</p>

One of the more apparent security issues is that you have to trust the
nameservers you're querying. If a malicious actor manages to compromise a
nameserver, they can return false responses to your queries. Since your machine
has no other way to verify the authoritative answer, it will happily connect to
whatever IP address the malicious nameserver returned. This is known as a
[DNS spoofing](https://en.wikipedia.org/wiki/DNS_spoofing) attack.

In more recent years, various solutions have been created to address this and
other shortcomings:
[DNSSEC](https://en.wikipedia.org/wiki/Domain_Name_System_Security_Extensions)
ensures the integrity of responses to your query, and DNS-over-TLS (DoT) and
DNS-over-HTTPS (DoH) take this a step further and prevent eavesdroppers from
seeing which queries you perform.

#### DNS enumeration

Ok, these solutions work great for protecting the answers to your queries, but
what about protecting the nameservers from the queries themselves?

If an organization defines records for its internal services, for example,
anyone can query its authoritative nameservers to map out a list of all the
organization's internal services. How convenient!

This is known as **DNS enumeration**, and is typically one of the first steps
taken by a malicious actor looking for potential entrypoints into an
organization.

Because of this issue, organizations often resort to running their own upstream
resolvers, configured to return different results (or sometimes none at all)
depending on the _source_ of the request. This technique is known as **Split
DNS**.

So how does it work?

## How Split DNS works

Split DNS is a technique wherein an orgnization maintains two (or more) separate
DNS servers (or a single server configured with two "zones") -- one for internal
resources and one for external resources. The internal server is only accessible
to users who are connected to an organization's internal network (such as a VPN
or branch office network), while the external zone is accessible to the outside
world.

<p className="text-sm italic text-center mx-auto">
  Figure 5: Split DNS in action
</p>

As an example, let's say an organization has an internal service called
`gitlab.company.com` which resolves to `10.10.10.10`. The organization's
internal DNS server would be configured to respond to queries for this service,
but the external DNS server would simply return `NXDOMAIN`, DNS speak for
"Sorry, wrong number". This allows the organization to publish some records
publicly, like `www.company.com` so that its website is accessible to the
public, while leaving the addresses of its private resources a secret.

Sprinkle in a little bit of matching DNS configuration onto your workforce's
machines to make sure the right server is being queried for the right domains,
and you're set.

Split DNS is a great building block for organizations looking to secure access
to their own internal applications, and continues to be a popular way to
mitigate enumeration attacks today.

So far, so good, right?

### The problem with Split DNS

Split DNS works great when you have a clear distinction between external and
internal resources. It allows you to publish public addresses for your public
resources so anyone can access them, and publish private addresses for your
workforce so they can connect to the private resources that you manage.

Increasingly, however, cloud-delivered services are replacing their on-premise
equivalents across many organizations. The upside here is generally lower
operational cost -- pay the businesses making the software to host it for you as
opposed to hosting and managing it yourself, and reap the efficiency benefits in
the form of cost reduction.

But the downside is resources that were once internal are now publicly exposed.
Anyone with the right credentials can access your organization's private
repository, CRM data, or CI/CD secrets from anywhere in the world. Without
internal addresses to resolve to, Split DNS doesn't help us anymore.

Is there another way to secure access to these services?

#### A naive solution

As it turns out, there's a simple solution to this problem that's becoming more
and more common across popular SaaS providers: IP allowlists. Many public SaaS
apps like GitHub, Slack, and Hubspot now allow you to configure a list of
allowed source addresses that can sign into the service.

<p className="text-sm text-center mx-auto italic">
  Figure 5: Example IP allowlist configuration
</p>

Now, the studious reader will have already recognized the solution to our public
exposure problem: route your workforce's traffic for these services through a
VPN jumphost or office router, egressed through a static IP that you've to your
SaaS provider's allowlist, and problem solved, right?

Well, mostly. But there's a catch: **virtual hosting**.

[Virtual hosting](https://en.wikipedia.org/wiki/Virtual_hosting) is a technique
used to host multiple services at a single IP address. It's become an essential
tool in the arsenl to fight IPv4 address exhaustion, and is increasingly being
used in IPv6 networks as well.

<p className="text-sm text-center mx-auto italic">
  Figure 6: Collateral damage can occur if you naively tunnel resolved IPs
</p>

If we simply resolve `github.com` and then configure our VPN to route traffic
for that IP address through a jumphost, we might inadvertently route traffic
`api.github.com`, `gist.github.com`, or `raw.githubusercontent.com` as well!

#### NAT to the rescue?

So we can't just blindly route traffic for the resolved IPs of the services
we're trying to secure. We need to translate them somehow. Enter NAT: Network
Address Translation.

We can solve the problem above by intecepting the DNS query to the service,
generating a unique IP address for it, and then adding a bit of NAT to convert
the generated IP address back to the actual IP address of the service.

This solves the collateral damange problem of routing traffic for the wrong
service, but it introduces a new problem: we need a way to intercept DNS queries
for the services we're trying to secure, generate a unique IP for them
on-the-fly, and then somehow route the resulting traffic through a NAT gateway
to the actual service.

<p className="text-sm text-center mx-auto italic">Figure 7:</p>

Seems complicated doesn't it? Keeping all of this in sync and up-to-date would
be a configuration nightmare. So we've taken what was originally a DNS problem
and translated it to a configuration problem.

## DNS-based traffic routing

And that's where Firezone comes in.

Firezone's approach to DNS was designed to address both the insecurity of DNS
and the shortcomings of Split DNS, resulting in a single solution that's just as
secure as Split DNS, but more flexible, and easier to manage, and best of all --
works with cloud-delivered services too. Let's see how.

### How it works

Remember the stub resolver we talked about earlier? Recall that it's a small
piece of software on your host that's responsible for resolving DNS queries.
It's the first stop for any DNS query made by an application on your host.

Well, each Firezone client embeds a tiny, lightweight stub resolver with some
interesting properties.

For DNS queries that don't match any of your Firezone-defined Resources, the
resolver operates like any other stub resolver, forwarding the query to your
upstream resolver as if Firezone didn't exist.

<p className="text-sm text-center mx-auto italic">
  Figure 8: Firezone's embedded stub resolver
</p>

However, for DNS queries that match a Resource you've defined in your Firezone
account, the resolver behaves a bit differently. Instead of forwarding the query
to your upstream resolver, it asks the Policy Engine first to authorize the
query for the Resource in question.

If approved, the Policy Engine forwards the query to a Gateway that's been
assigned to serve the Resource. The Gateway then resolves the query and returns
the IP address of the Resource to the stub resolver.

<Alert color="info">
  Gateways are lightweight Firezone servers that run on your infrastructure and
  connect your users to your defined Resources. Read more about them in our{" "}
  <Link href="/kb/deploy/gateways">Gateway documentation</Link>.
</Alert>

Here's where things get interesting: instead of returning the actual IP address
of the Resource to the application that made the query, the stub resolver
_returns a dummy IP address that it generated on-the-fly_. This dummy IP address
is not only unique to the Resource and query in question, but also unique to the
Gateway and Site that contains the Gateway as well.

This means that two Resources that might resolve to the same actual address will
resolve to different addreses if they're served by two different Gateways.

All of this happens in under a second, completely transparently to the user.

If you assign your Gateway a static IP address, you can now add that IP address
to your SaaS provider's IP allowlist, and your workforce can access the service
securely from anywhere in the world.

A nice side effect of this approach is that it _also_ works for Split DNS.

### How it's implemented

When a user signs in, the Firezone Client configures its host operating system
to use a special, lightweight DNS proxy running inside the Client for all DNS
queries on the system.

This proxy lives at `100.100.111.0/24` for IPv4 queries and
`fd00:2021:1111:8000:100:100:111:0/120` for IPv6 queries. These addresses are
not routable on the public internet, so there's no risk that queries for
internal resources will leave the device unexpectedly.

The IP addresses the proxy chooses to "listen on" are determined by the
[upstream resolvers](#configuring-client-dns-upstream-resolvers) available to
the Client. For each IPv4 upstream resolver, the proxy listens on an address in
the `100.100.111.0/24` range, and for each IPv6 upstream resolver, the proxy
listens on an address in the `fd00:2021:1111:8000:100:100:111:0/120` range.

<p className="text-sm text-center mx-auto italic">
  Figure 9: How multiple DNS servers are handled
</p>

When an application makes a DNS query, the proxy checks if the name matches a
Firezone Resource that the user has access to. If it does, the Client then
requests a Gateway serving that Resource to resolve the DNS query. When the
Gateway responds, the proxy will create a temporary mapping between the actual
IP address of the Resource and the dummy IP address it returned to the
application. From this point onward, all subsequent packets to the dummy IP
address are forwarded to the Gateway that resolved the query. These IPs are
persisted for the duration of the Client's session.

```
user@host:~ % dig ifconfig.net

; <<>> DiG 9.10.6 <<>> ifconfig.net
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 51756
;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;ifconfig.net.			IN	A

;; ANSWER SECTION:
ifconfig.net.		300	IN	A	100.96.0.2
ifconfig.net.		300	IN	A	100.96.0.1

;; Query time: 262 msec
;; SERVER: fd00:2021:1111:8000:100:100:111:0#53(fd00:2021:1111:8000:100:100:111:0)
;; WHEN: Sat Feb 17 14:04:40 PST 2024
;; MSG SIZE  rcvd: 86
```

<p className="text-sm text-center mx-auto italic">
  Figure 10: An example dig command with a DNS-based Resource
</p>

If the proxy sees a query for a name that **is not** a Firezone Resource it will
forward the query to the host's default stub resolver, as if Firezone never
existed. This means that Clients are automatically configured for Split DNS in
Firezone -- no other configuration is necessary other than adding the desired
Resources in the admin portal.

{/* TODO: Add much more detail here */}

## Conclusion

There's much more detail we could have covered here, so if you're interested in
seeing how this works under the hood, all source code is available -- go
[take a look](https://github.com/firezone/firezone/blob/main/rust/connlib/tunnel/src/dns.rs)
for yourself!

### References

1. _Domain Names - Concepts and Facilities_,
   https://datatracker.ietf.org/doc/html/rfc882
