import Image from "next/image";
import Alert from "@/components/DocsAlert";
import Link from "next/link";

```
user@host:~ % dig slack.com

; <<>> DiG 9.10.6 <<>> slack.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 61288
;; flags: qr rd ra; QUERY: 1, ANSWER: 9, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;slack.com.			IN	A

;; ANSWER SECTION:
slack.com.		1	IN	A	100.96.0.93
slack.com.		1	IN	A	100.96.0.97
slack.com.		1	IN	A	100.96.0.94
slack.com.		1	IN	A	100.96.0.92
slack.com.		1	IN	A	100.96.0.98
slack.com.		1	IN	A	100.96.0.90
slack.com.		1	IN	A	100.96.0.96
slack.com.		1	IN	A	100.96.0.91
slack.com.		1	IN	A	100.96.0.95

;; Query time: 59 msec
;; SERVER: 100.100.111.2#53(100.100.111.2)
;; WHEN: Fri May 03 09:31:09 PDT 2024
;; MSG SIZE  rcvd: 252
```

Firezone's approach to DNS works a bit differently than one might expect. One
question we get a lot is, "why do my DNS Resources resolve to different IPs with
Firezone enabled?". Great question. Let's explain that now!

What follows is a brief history of DNS, (a few of) the security issues that
plague it, and how Firezone's DNS-based traffic routing was designed to address
them.

## A brief history lesson

But before we dive into all the fun details, it's helpful to first understand
the problem DNS was originally designed to solve.

DNS was created in the early 1980s to address a classic scalability problem with
the early ARPANET.

<Image
  src="/images/blog/how-dns-based-traffic-routing-works/arpanet-logical-map.png"
  alt="Early ARPANET logical map"
  width={1200}
  height={1200}
  className="mx-auto rounded shadow"
/>

<p className="text-sm italic text-center mx-auto">
  Figure 1: Early ARPANET logical map
</p>

The [ARPANET](https://en.wikipedia.org/wiki/ARPANET), the precursor to the
modern internet, was a network of computers that connected various research
institutions and government agencies across the United States. It was the first
network to use the new TCP/IP protocol suite, which provided an IP address to
each host on the network.

To connect to a host on the ARPANET, you needed its IP address. But there was a
problem: it turns out humans aren't great at remembering long strings of
numbers. To solve this, a simple system was created to map each host's IP
address to a human-friendly "hostname".

<p className="text-sm italic text-center mx-auto">
  Figure 2: Artist's rendition of the original HOSTS.TXT file
</p>

Hostname to IP address mappings were stored in a single file, aptly named
`HOSTS.TXT`, and stored on the `SRI-NIC` server -- a system maintained by the
Network Information Center (NIC) at the Stanford Research Institute (SRI).

Simple enough, right? Well, not quite. The system was perhaps a bit _too_
simple.

### Files don't scale

Whenever an ARPANET member organization (yes, you had to be a member at that
time) wanted to add a host to the file or update its IP address, it had to
contact the SRI (during standard business hours!) and file a formal request
containing the updated mapping.

Need to make an update over the weekend or on a US holiday? Too bad, you were
out of luck! The system worked ok when there were only a handful of hosts to
maintain. But, as you might imagine, it became increasingly unmanageable as more
hosts joined.

## Enter the Domain Name System (DNS)

Over the course of several meetings, early ARPANET engineers (who would later go
on to form the [IETF](https://www.ietf.org/)) devised a clever solution to the
scalability problem they were facing: instead of storing the hostname lookup
table in a single file, they would chop it up instead and distribute the entries
across several "Nameservers", each one responsible for maintaining the hosts
that belonged to a particular network, or "Domain".

Hence the name: the Domain Name System, or DNS for short.

### How DNS works

At a high level, DNS is a hierarchical system that distributes the
responsibility of resolving a fully-qualified domain name (FQDN) to a series of
nameservers, each one responsible for resolving a different part.

<p className="text-sm italic text-center mx-auto">Figure 3: How DNS works</p>

Here's how it works:

1. An application makes a query. The first stop is the **stub resolver**, a
   small piece of software on the host that's responsible for resolving DNS
   queries.
1. The stub resolver typically maintains a small cache of recent queries. If the
   query misses the cache, it forwards the query to an **upstream resolver**.
   This is typically a larger, caching resolver run by your ISP or more recently
   a public DNS service like NextDNS or Cloudflare.
1. If _this_ query misses the cache, the upstream resolver begins the full
   process of resolution. It starts by forwarding the query to a **root
   nameserver**. The root nameserver is at the top of the DNS hierarchy.
1. The root nameserver responds to the upsteam resolver with the IP address of
   the **TLD nameserver** for the root in question, for example `com` or `net`.
   The upstream resolver then forwards the query to the TLD nameserver.
1. The TLD nameserver responds with the IP address of the **authoritative
   nameserver** for the domain in question. This is the nameserver the owner of
   the domain will configure to respond to queries for that domain. It's where
   you typically configure your DNS records for your domain, for example.
1. The upstream resolver then forwards the query to the authoritative
   nameserver.
1. Finally, the authoritative nameserver responds with the IP address of the
   host in question, and the upstream resolver returns the final answer to the
   stub resolver on the host that originally made the query.
1. The application on the host can now connect to the IP address returned by the
   stub resolver.

The system of root, TLD, and authoritative nameservers replace the function of
the original `HOSTS.TXT` file maintained by NIC in the ARPANET. However, now
it's distributed -- no one server or organization is responsible for maintaining
the entire database, and the system can scale as more hosts, domains, and IP
addresses are added.

On today's internet, the whole process for resolving a query usually takes only
a few hundred milliseconds. Caching resolvers help to speed up the process by
storing the results of queries for a certain amount of time, known as the
record's time-to-live (TTL).

This means that if a host makes the same query multiple times, the stub or
upstream resolver can return the result immediately (assuming the TTL hasn't
expired) without having to query the hierarchy of root, TLD, and authoritative
nameservers again. This can speed up query times by orders of magnitude, to the
point where stub resolvers responding with cached responses are nearly
instantaneous.

So hostnames are now mapped to IP addresses in a scalable way, and queries are
pretty fast on average. DNS worked quite well for the ARPANET! So well, in fact,
that it was adopted mostly unchanged by the internet as it grew to become the
global network we know and love today.

But hold your champagne bottles. There's just one problem with all these
delightfully distributed domain lookups: security.

### Security issues with DNS

The thing is, DNS was designed in the early 1980s, when the ARPANET was a small,
trusted network of research institutions and government agencies. The system was
designed with the assumption that all entities on the network were known and
trusted, and that the network itself was secure.

As the ARPANET grew to become the internet, however, this assumption no longer
held.

#### DNS spoofing

<p className="text-sm italic text-center mx-auto">
  Figure 4: An illustration of DNS spoofing
</p>

One of the more apparent security issues is that you have to trust the
nameservers you're querying. If a malicious actor manages to compromise a
nameserver, they can return false responses to your queries. Since your machine
has no other way to verify the authoritative answer, it will happily connect to
whatever IP address the malicious nameserver returned. This is known as a
[DNS spoofing](https://en.wikipedia.org/wiki/DNS_spoofing) attack.

In more recent years, various solutions have been created to address this and
other shortcomings:
[DNSSEC](https://en.wikipedia.org/wiki/Domain_Name_System_Security_Extensions)
ensures the integrity of responses to your query, and DNS-over-TLS (DoT) and
DNS-over-HTTPS (DoH) take this a step further and prevent eavesdroppers from
seeing which queries you perform.

#### DNS enumeration

Ok, these solutions work great for protecting the answers to your queries, but
what about protecting the nameservers from the queries themselves?

If an organization defines records for its internal services, for example,
anyone can query its authoritative nameservers to map out a list of all the
organization's internal services. How convenient!

This is known as **DNS enumeration**, and is typically one of the first steps
taken by a malicious actor looking for potential entrypoints into an
organization.

Because of this issue, organizations often resort to running their own upstream
resolvers, configured to return different results (or sometimes none at all)
depending on the _source_ of the request. This technique is known as **Split
DNS**.

## How Split DNS works

[Split DNS](https://en.wikipedia.org/wiki/Split-horizon_DNS) is a technique
wherein an orgnization maintains two (or more) separate DNS servers (or a single
server configured with two "zones") -- one for internal resources and one for
external resources. The internal server is only accessible to users who are
connected to an organization's internal network (such as a VPN or branch office
network), while the external zone is accessible to the outside world.

<p className="text-sm italic text-center mx-auto">
  Figure 5: Split DNS in action
</p>

As an example, let's say an organization has an internal service called
`gitlab.company.com` which resolves to `10.10.10.10`. The organization's
internal DNS server would be configured to respond to queries for this service,
but the external DNS server would simply return `NXDOMAIN`, DNS speak for
"Sorry, wrong number". This allows the organization to publish some records
publicly, like `www.company.com` so that its website is accessible to the
public, while leaving the addresses of its private resources a secret.

Sprinkle in a little bit of matching DNS configuration onto your workforce's
machines to make sure the right server is being queried for the right domains,
and you're set.

Split DNS is a great building block for organizations looking to secure access
to their own internal applications, and continues to be a popular way to
mitigate enumeration attacks today.

So far, so good, right?

### The problem with Split DNS

Split DNS works great when you have a clear distinction between external and
internal resources. It allows you to publish public addresses for your public
resources so anyone can access them, and publish private addresses for your
workforce so they can connect to the private resources that you manage.

Increasingly, however, cloud-delivered services are replacing their on-premise
equivalents across many organizations. The upside here is generally lower
operational cost -- pay the businesses making the software to host it for you as
opposed to hosting and managing it yourself, and reap the efficiency benefits in
the form of cost reduction.

But the downside is resources that were once internal are now publicly exposed.
Anyone with the right credentials can access your organization's private
repository, CRM data, or CI/CD secrets from anywhere in the world. Without
internal addresses to resolve to, Split DNS doesn't help us anymore.

Is there another way to secure access to these services, now that we can't hide
them with Split DNS?

#### A naive solution

As it turns out, there's a simple solution to this problem that's becoming more
and more common across popular SaaS providers: IP allowlists. Many public SaaS
apps like GitHub, Slack, and Hubspot now allow you to configure a list of
allowed source addresses that can sign into the service.

<p className="text-sm text-center mx-auto italic">
  Figure 5: Example IP allowlist configuration
</p>

Now, the studious reader will have already recognized the solution to our public
exposure problem: route your workforce's traffic for these services through a
VPN jumphost or office router, egressed through a static IP that you've to your
SaaS provider's allowlist, and problem solved, right?

Well, mostly. But there's a catch: **virtual hosting**.

[Virtual hosting](https://en.wikipedia.org/wiki/Virtual_hosting) is a technique
used to host multiple services at a single IP address. It's become an essential
tool in the arsenal to fight IPv4 address exhaustion, and is increasingly used
in IPv6 networks as well.

<p className="text-sm text-center mx-auto italic">
  Figure 6: Collateral damage can occur if you naively tunnel resolved IPs
</p>

If we simply resolve `github.com` and then configure our VPN to route traffic
for that IP address through a jumphost, we might inadvertently route traffic for
`api.github.com`, `gist.github.com`, or `raw.githubusercontent.com` as well!

#### NAT to the rescue?

So we can't just blindly route traffic for the resolved IPs of the services
we're trying to secure. We need to translate them somehow. Enter NAT: Network
Address Translation.

We can solve the problem above by intecepting the DNS query to the service,
generating a unique IP address for it, and then adding a bit of NAT to convert
the generated IP address back to the actual IP address of the service.

This solves the collateral damange problem of routing traffic for the wrong
service, but it introduces a new problem: we need a way to intercept DNS queries
for the services we're trying to secure, generate a unique IP for them
on-the-fly, and then somehow route the resulting traffic through a NAT gateway
to the actual service.

<p className="text-sm text-center mx-auto italic">
  Figure 7: DNS interception + NAT gateway: problem solved?
</p>

Seems complicated doesn't it? Keeping all of this in sync and up-to-date would
be a configuration nightmare. So we've taken what was originally a DNS problem
and translated it to a configuration problem.

## DNS-based traffic routing

That's where Firezone comes in.

Firezone's approach to DNS was designed to combine the benefits of Split DNS for
internal services with the routing benefits for IP-allowlisted public services.
Let's see how.

### How it works

Remember the stub resolver we talked about earlier? Recall that it's a small
piece of software on your host that's responsible for resolving DNS queries.
It's the first stop for any DNS query made by an application on your host.

Well, each Firezone client embeds a tiny, lightweight stub resolver that works
just like the one your operating system provides, but with a special twist.

For DNS queries that don't match any of your Firezone-defined Resources, the
resolver operates like any other stub resolver, forwarding the query to your
upstream resolver as if Firezone didn't exist.

For DNS queries that match a defined Resource, however, the resolver behaves a
bit differently.

Instead of forwarding the query to your upstream resolver, it first asks
Firezone's Policy Engine to authorize the query for the Resource in question.

If approved, the Policy Engine forwards the query to a Gateway that's available
to serve the Resource. The Gateway then resolves the query for the Resource
(using _its_ stub resolver) and returns the result back to our special stub
resolver in the Client.

<Alert color="info">
  Read more about [Gateways](/kb/architecture/core-components#gateways) and the
  [Policy Engine](/kb/architecture/core-components#policy-engine) in our
  architecture docs.
</Alert>

Here's where things get interesting: instead of passing the resolved IP address
as-is back to the application, the stub resolver generates a dummy IP address
for it and returns that instead.

<p className="text-sm text-center mx-auto italic">
  Figure 8: Firezone's embedded stub resolver
</p>

Now, when as the application sends packets to the dummy IP, they're routed
through the newly established Firezone tunnel to the Gateway that resolved the
query. The Gateway then forwards this traffic on to the public service, setting
the source address to the static IP we've configured in the service's allowlist,
and we've now secured access to the public service using its DNS name.

All of this happens in about the same time it would take for a query to be
resolved without Firezone, so the end user is none the wiser. Our public
services from above can now be selectively routed through Firezone no matter how
many other services may be hosted at the same IP!

### How it's implemented

We glossed over lots of details above. The section below will get a bit
technical, so if you're not interested in the nitty-gritty details, feel free to
skip ahead to the [conclusion](#conclusion). If you are, well, let's dive
deeper.

#### Query interception

The process actually starts when a user signs in to the Firezone Client. When
this happens, the Client reads which upstream resolvers are available on the
host, generates corresponding "sentinel addresses" for each one, and then
configures the host's operating system to use these as the host's default
upstream resolvers instead. This is how the sub resolver mentioned above is able
to intercept queries to act on as needed.

For each IPv4 and IPv6 upstream resolver it finds on the host, the Client
generates a matching sentinel address in the `100.100.111.0/24` and
`fd00:2021:1111:8000:100:100:111:0/120` respectively. This is why you'll often
see `100.100.111.1` set as your upstream resolver in `/etc/resolv.conf` with the
Firezone Client signed in.

A nice side effect of this one-to-one mapping approach is that it won't affect
the nameserver selection algorithm in the host's operating system. If an
upstream resolver is down, the corresponding sentinel address will be
unresponsive, and the operating system will pick another, responsive one to use
instead.

#### Generating the mappings

- Multiple Resource IPs
- Gateways that resolve different Resource IPs

{/* TODO: Add much more detail here */}

## Conclusion

There's much more detail we could have covered here, so if you're interested in
seeing how this works under the hood, all source code is available -- go
[take a look](https://github.com/firezone/firezone/blob/main/rust/connlib/tunnel/src/dns.rs)
for yourself!

### References

1. _Domain Names - Concepts and Facilities_,
   https://datatracker.ietf.org/doc/html/rfc882
