Firezone's approach to DNS works a bit differently than one might expect. One
question we get a lot is, "why do my DNS Resources resolve to these weird IPs
with Firezone enabled?". The short answer is: we proxy DNS. The long answer:
well, that's what we're here to explain!

This post brings to light a few DNS security challenges faced by organizations,
a few ways to solve them, and finally how Firezone's DNS-based traffic
redirection solves them in a new way.

Ready? Let's get started.

## A brief history lesson

To grasp some of the security issues that can arise from poorly configured DNS,
we first need to understand the problem DNS was built to solve. Time for a
history lesson!

The Domain Name System (herein referred to simply as "DNS") was originally
conceived to solve a classic scalability problem:

Hosts on the ARPANET (the precursor to the modern internet) were addressed using
the new IPv4 addressing scheme. To avoid having to remember the IP address for
each host you wanted to connect to, a centralized database was used to store a
mapping between every host's IPv4 address and a human-friendly hostname.

Where was this database stored, and how was it updated?

The database for this early system was literally just a single text file (aptly
named `HOSTS.TXT`) stored on a central system managed jointly by the Stanford
Research Institute (SRI) and Network Information Center (NIC) [1].

<p className="text-sm italic text-center mx-auto">
  Figure 1: Artist's rendition of the original HOSTS.TXT file
</p>

Whenever an ARPANET member organization (yes, you had to be a member at that
time to join ARPANET) wanted to add a host to the file or update its IP address,
it needed to contact the SRI (during standard business hours!) and file a
request containing the updated information.

Need to make an update over the weekend? Too bad, you were out of luck.

The system worked great when there were only a handful of hosts to maintain.
But, as you might imagine, it quickly started to become unmanageable as more and
more organizations connected their machines to the ARPANET.

The problem was especially apparent with computer mail (known today as email).
The `user@host` convention was just beginning to take hold, but without a domain
appended to the host, computer mail providers resorted to creating "an
increasingly large and irregular set of methods for identifying the location of
a mailbox" [2]. Not only were email providers dealing with the naming issue for
hosts on the ARPANET, they also needed a standardized way to identify _user
mailboxes_ on those hosts.

It was clear a solution was needed, and fast -- ARPANET's growth wasn't slowing
down anytime soon.

## Enter the Domain Name System (DNS)

Over the course of several meetings, engineers from what would eventually become
the IETF devised a clever solution to the scalability problem they were facing:
instead of storing hostnames and IPs in a single, central `HOSTS.TXT` file, they
would instead be split and stored across multiple "Name Servers", each one
responsible for maintaining hosts that belonged to a particular network, or
"Domain".

This is in fact where DNS gets its name -- it is quite literally the Domain Name
System.

### How DNS works

So how does it work?

DNS is a heirarchical system that distributes the responsibility of resolving a
a fully-qualified domain name (FQDN) to a series of nameservers.

<p className="text-sm italic text-center mx-auto">Figure 2: How DNS works</p>

1. The first stop for a query is typically the stub resolver on the host itself.
   This is a small piece of software included with most operating systems and is
   responsible for sending DNS queries it can't answer to an upstream resolver.
1. If the stub resolver doesn't know the answer to the query, it forwards the
   query to an _upstream resolver_. This is typically a caching resolver that
   has a list of root nameservers it can query.
1. If the upstream resolver doesn't the query in its cache, it forwards the
   query to a root nameserver. The root nameserver is the top of the DNS
   hierarchy and knows the IP addresses of the nameservers for each top-level
   domain (TLD).
1. The root nameserver responds to the upsteam resolver with the IP address of
   the TLD nameserver for the root in question, for example `COM` or `NET`. The
   upstream resolver then forwards the query to the TLD nameserver.
1. The TLD nameserver responds with the IP address of the authoritative
   nameserver for the domain in question. This is the nameserver the owner of
   the domain will configure to respond to queries for that domain. It's where
   you typically configure your DNS records for your domain, for example.
1. The upstream resolver then forwards the query to the authoritative
   nameserver.
1. Finally, the authoritative nameserver responds with the IP address of the
   host in question (assuming an A or AAAA query), and the upstream resolver
   returns this to the stub resolver on the host that made the original query.
1. The application on the host can now connect to the IP address returned by the
   stub resolver.

The system of root, TLD, and authoritate nameservers replace the function of the
original "HOSTS.TXT" file maintained by NIC in the ARPANET. However, now it's
distributed -- no one server or organization is responsible for maintaining the
entire database, and the system can scale as more hosts, domains, and IP
addresses are added.

On today's internet, the whole process for resolving an query usually takes only
a few hundred milliseconds. Caching resolvers help to speed up the process by
storing the results of queries for a certain amount of time, known as the
record's time-to-live (TTL).

This means that if a host makes the same query multiple times, the resolver can
return the result immediately without having to query the hierarchy of root,
TLD, and authoritative nameservers again. This can speed up query times orders
of magnitude, to the point where stub resolvers responding with cached responses
are nearly instantaneous.

So now names are mapped to IP addresses in a scalable way, and queries are
pretty fast on average. So far so good, right?

### Security issues with DNS

Well, as the ARPANET grew to become the internet we know and love today, a few
issues arose.

Remember above how we noted that only _member_ organizations of the ARPANET
could request updates to the `HOSTS.TXT` file? Well, now that the file is
effectively distributed over many servers, who manages updates? The domain owner
manages updates to their domain via the authoritative server, but how are we
sure that the upstream resolver is responding with the correct IP addresses?

The answer is: we don't.

Since all entities on the ARPANET were known and trusted at the time, this was
largely a non-issue. As the ARPANET grew to become the internet we all know and
love today, however, this became a problem.

<p className="text-sm italic text-center mx-auto">
  Figure 3: An illustration of DNS spoofing
</p>

Over the years, however, security issues started to become apparent. One obvious
one is that you have to trust the nameservers you're querying. If a malicious
actor manages to compromise a nameserver, they can return false responses to
your queries. This is known as a _DNS spoofing_ attack.

Since your machine has no other way to verify the authoritative answer, it will
happily connect to whatever IP address the malicious nameserver returned.

Another major issue is the lack of encryption in the DNS protocol. DNS queries
are sent in plaintext, which means that anyone who can intercept your network
traffic can see what websites you're visiting. This is the privacy disaster
known as a _DNS eavesdropping_ attack.

In more recent years, various solutions have been created to address these
shortcomings: DNSSEC ensures the integrity of responses to your query, and
encrypted solutions like DNS-over-TLS (DoT) and DNS-over-HTTPS (DoH) take this a
step further and prevent eavesdroppers from seeing which queries you perform.

But there's a still a big problem with DNS that isn't directly addressed by
these solutions. Queries are unauthenticated: what if the upstream resolver
wants to ensure that only certain parties or entities can query it? What if an
organization wants to restrict which records are returned by its authoritative
nameserver based on who's asking?

If an organization defines a DNS record for an internal service, for example,
anyone on the outside world can query its nameservers and map out the internal
network topology. This is known as a _DNS enumeration_, and is typically one of
the first steps taken by a malicious actor looking for potential entrypoints
into an organization.

Because of this issue, organizations often resort to running their own upstream
resolvers, configured to return different results (or sometimes none at all)
depending on the source of the request. This technique is known as _Split DNS_,
and remains a popular way to mitigate enumeration attacks today.

So how does it work?

## How Split DNS works

Split DNS is a technique wherein an orgnization maintains two (or more) separate
DNS servers (or a single server configured with two "zones") -- one for internal
resources and one for external resources. The internal server is only accessible
to users who are connected to an organization's internal network (such as a VPN
or branch office network), while the external zone is accessible to the outside
world.

<p className="text-sm italic text-center mx-auto">
  Figure 4: Split DNS in action
</p>

As an example, let's say an organization has an internal service called
`gitlab.company.com` which resolves to `10.10.10.10`. The organization's
internal DNS server would be configured to respond to queries for this service,
but the external DNS server would simply return `NXDOMAIN`, DNS speak for
"Sorry, wrong number". This allows the organization to publish some records
publicly, like `www.company.com` so that its website is accessible to the
public, while leaving the addresses of its private resources a secret.

Sprinkle in a little bit of matching DNS configuration onto your workforce's
machines to make sure the right server is being queried for the right domains,
and you're set.

Split DNS works well for organizations that want to secure access to their own
internal applications, and continues to be a popular choice for many companies.

So far, so good right?

### The problem with Split DNS

Split DNS works great when you have a clear distinction between external and
internal resources. It allows you to publish public addresses for your public
resources so anyone can access them, and publish private addresses for your
workforce so they can connect to the private resources that you manage.

Increasingly, however, cloud-delivered services are replacing their on-premise
equivalents across many organizations. The upside here is generally lower
operational cost -- pay the businesses making the software to host it for you as
opposed to hosting and managing it yourself, and reap the efficiency benefits in
the form of cost reduction.

But with these public SaaS apps, the downside is you can't use Split DNS to
protect them anymore -- they aren't running on your infrastructure, so there
simply aren't any internal zones and therefore no internal IPs to give to your
workforce to connect to.

As it turns out, there's a simple solution to this problem that's becoming more
and more common across popular SaaS providers. Many public SaaS apps like
GitHub, Slack, and Hubspot provide an IP allowlist feature that limits access to
your tenant based on a range of allowed source addresses.

Now, the intrepid reader will be quick to recognize the solution to our Split
DNS problem: can't the organization simply tunnel the resolved IP addresses of
these services through the VPN, just like they did before with internal
services?

And yes, that _can_ work in _some_ circumstances, but there's one minor problem
ruining all our fun:

If the service uses Name-based virtual hosting, for example, the IP address that
the DNS query resolves to is shared among many different services, and not just
the one the organization is interested in.

<p className="text-sm text-center mx-auto italic">
  Figure 5: Name-based virtual hosting collateral damage
</p>

This means that an organization wishing to route DNS traffc _only_ for
`github.com` might inadvertently route traffic for _other_ GitHub services
running on the same IPs!

Ok, but what about another solution: can the organization simply configure its
client devices to use the organization's DNS server to respond with internal IP
addresses for the public services?

That's much better -- it solves the problem of routing traffic for the wrong
service, but it introduces a new problem: the organization now needs to maintain
a list of all the public services its workforce needs and configure its DNS
server to respond with a unique internal IP address for each one, and then be
sure these IP addresses don't conflict with other addresses that may be
available on the employee's machine, such as home printers and such.

On top of the addressing headache, the organization needs to operate NAT routers
that accept the traffic from the VPN clients across their workforce, translate
the source addresses so they appear to come from one of the IPs in the
allowlist, and finally forward the traffic on to the service in question.

The IP addressing configuration and NAT router configuration would need to be
kept in sync so that they function together. So we've taken what was oringally a
DNS problem and translated it to a configuration management problem.

Ok, slightly better, but still a burden for organizations to manage at scale.

## DNS-based traffic redirection

That's where Firezone comes in.

Firezone's DNS routing system was designed to address both the insecurity of
DNS, and the shortcomings of Split DNS, into a single solution that's just as
secure as Split DNS, but much more flexible and easier to manage.

### How it works

When a user signs in, the Firezone Client configures its host operating system
to use a special, lightweight DNS proxy running inside the Client for all DNS
queries on the system.

This proxy lives at `100.100.111.0/24` for IPv4 queries and
`fd00:2021:1111:8000:100:100:111:0/120` for IPv6 queries. These addresses are
not routable on the public internet, so there's no risk that the queries will
leave the device unexpectedly.

```
user@host:~ % dig ifconfig.net

; <<>> DiG 9.10.6 <<>> ifconfig.net
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 51756
;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;ifconfig.net.			IN	A

;; ANSWER SECTION:
ifconfig.net.		300	IN	A	100.96.0.2
ifconfig.net.		300	IN	A	100.96.0.1

;; Query time: 262 msec
;; SERVER: fd00:2021:1111:8000:100:100:111:0#53(fd00:2021:1111:8000:100:100:111:0)
;; WHEN: Sat Feb 17 14:04:40 PST 2024
;; MSG SIZE  rcvd: 86
```

<p className="text-sm text-center mx-auto italic">
  Figure 6: An example dig command with a DNS-based Resource
</p>

The IP addresses the proxy chooses to "listen on" are determined by the
[upstream resolvers](#configuring-client-dns-upstream-resolvers) available to
the Client. For each IPv4 upstream resolver, the proxy listens on an address in
the `100.100.111.0/24` range, and for each IPv6 upstream resolver, the proxy
listens on an address in the `fd00:2021:1111:8000:100:100:111:0/120` range.

When an application makes a DNS query, the proxy checks if the name matches a
Firezone Resource that the user has access to. If it does, the Client then
requests a Gateway serving that Resource to resolve the DNS query. When the
Gateway responds, the proxy will create a temporary mapping between the actual
IP address of the Resource and the dummy IP address it returned to the
application. From this point onward, all subsequent packets to the dummy IP
address are forwarded to the Gateway that resolved the query. These IPs are
persisted for the duration of the Client's session.

immediately responds with appropriate dummy IPs in the `100.64.96.0/11` and
`fd00:2021:1111:8000/108` range and returns these to the application making the
query. These IPs are remembered for the duration of the Client's session and
used to intelligently route subsequent packets to a Firezone Gateway suitable
for serving that Resource.

If the proxy sees a query for a name that **is not** a Firezone Resource that
the user has access to, it will forward the query to the upstream DNS server(s)
configured by the admin in `Settings` -> `DNS`. If no upstream DNS servers are
configured, the query is forwarded to the Client device's default system
resolver(s) instead.

This means that Clients are automatically configured for Split DNS in Firezone
-- no other configuration is necessary other than adding the desired Resources
in the admin portal.

### References

1.
1. _Domain Names - Concepts and Facilities_,
   https://datatracker.ietf.org/doc/html/rfc882
1.
